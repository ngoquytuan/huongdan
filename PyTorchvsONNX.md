R√µ r·ªìi üëç Em s·∫Ω so·∫°n cho anh m·ªôt **t√†i li·ªáu ƒë·∫ßy ƒë·ªß** (ki·ªÉu h∆∞·ªõng d·∫´n k·ªπ thu·∫≠t) ƒë·ªÉ anh ho·∫∑c ƒë·ªìng nghi·ªáp c√≥ th·ªÉ ch·∫°y l·∫°i to√†n b·ªô qu√° tr√¨nh **so s√°nh PyTorch vs ONNX Runtime**.

---

# üìò H∆∞·ªõng d·∫´n: So s√°nh t·ªëc ƒë·ªô Inference gi·ªØa PyTorch v√† ONNX Runtime

## 1Ô∏è‚É£ Gi·ªõi thi·ªáu

Trong c√°c h·ªá th·ªëng AI/ML th·ª±c t·∫ø, PyTorch th∆∞·ªùng ƒë∆∞·ª£c d√πng trong **qu√° tr√¨nh hu·∫•n luy·ªán** v√† tinh ch·ªânh m√¥ h√¨nh. Tuy nhi√™n khi tri·ªÉn khai (deployment), ta c·∫ßn t·ªëc ƒë·ªô inference cao v√† g·ªçn nh·∫π ‚Üí ONNX Runtime l√† l·ª±a ch·ªçn t·ªëi ∆∞u.

T√†i li·ªáu n√†y h∆∞·ªõng d·∫´n:

* C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt.
* Vi·∫øt script benchmark m·ªôt m√¥ h√¨nh sentence embedding (`paraphrase-multilingual-MiniLM-L12-v2`).
* So s√°nh t·ªëc ƒë·ªô inference gi·ªØa PyTorch v√† ONNX Runtime.

---

## 2Ô∏è‚É£ C√†i ƒë·∫∑t m√¥i tr∆∞·ªùng

Y√™u c·∫ßu: Python 3.9+ (n√™n d√πng Python 3.10).

```bash
# T·∫°o virtualenv (khuy·∫øn ngh·ªã)
python -m venv venvONNX
.\venvONNX\Scripts\activate   # Windows
source venvONNX/bin/activate # Linux/Mac

# C√†i ƒë·∫∑t th∆∞ vi·ªán c·∫ßn thi·∫øt
pip install torch==2.2.2
pip install sentence-transformers
pip install transformers
pip install onnx onnxruntime
```

Ki·ªÉm tra l·∫°i:

```bash
python -c "import torch, onnxruntime; print(torch.__version__, onnxruntime.__version__)"
```

---

## 3Ô∏è‚É£ Script so s√°nh (PyTorchvsONNXRuntime.py)

L∆∞u file d∆∞·ªõi t√™n `PyTorchvsONNXRuntime.py`:

```python
import time
import torch
import onnx
import onnxruntime as ort
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer

# ----------- Config ------------
model_name = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
onnx_path = "model.onnx"

sentences = [
    "Xin ch√†o, t√¥i l√† AI.",
    "H√¥m nay tr·ªùi ƒë·∫πp qu√°.",
    "T√¥i th√≠ch h·ªçc m√°y."
]

# ----------- PyTorch Inference ------------
model_torch = SentenceTransformer(model_name)
start = time.time()
embeddings_torch = model_torch.encode(sentences, convert_to_numpy=True)
end = time.time()

print("üîπ PyTorch Inference")
print("Embeddings shape (PyTorch):", embeddings_torch.shape)
print(f"Time (PyTorch): {end - start:.4f} s")

# ----------- Export ONNX n·∫øu ch∆∞a c√≥ ------------
tokenizer = AutoTokenizer.from_pretrained(model_name)
try:
    onnx_model = onnx.load(onnx_path)
    print("‚ö° D√πng l·∫°i ONNX model ƒë√£ c√≥:", onnx_path)
except:
    print("‚ö° Exporting model sang ONNX...")
    model_onnx = model_torch._first_module().auto_model.cpu()
    dummy_input = tokenizer("C√¢u th·ª≠ nghi·ªám", return_tensors="pt")
    dummy_input = {
        "input_ids": dummy_input["input_ids"].cpu(),
        "attention_mask": dummy_input["attention_mask"].cpu()
    }
    torch.onnx.export(
        model_onnx,
        (dummy_input["input_ids"], dummy_input["attention_mask"]),
        onnx_path,
        input_names=["input_ids", "attention_mask"],
        output_names=["last_hidden_state"],
        dynamic_axes={
            "input_ids": {0: "batch", 1: "sequence"},
            "attention_mask": {0: "batch", 1: "sequence"},
            "last_hidden_state": {0: "batch", 1: "sequence"}
        },
        opset_version=14
    )
    print("‚úÖ Xu·∫•t ONNX th√†nh c√¥ng:", onnx_path)

# ----------- ONNX Inference ------------
ort_session = ort.InferenceSession(onnx_path, providers=["CPUExecutionProvider"])

tokens = tokenizer(sentences, padding=True, truncation=True, return_tensors="np")
start = time.time()
onnx_outputs = ort_session.run(
    ["last_hidden_state"],
    {"input_ids": tokens["input_ids"], "attention_mask": tokens["attention_mask"]}
)
end = time.time()

embeddings_onnx = onnx_outputs[0].mean(axis=1)  # pooling
print("\nüîπ ONNX Inference")
print("Embeddings shape (ONNX):", embeddings_onnx.shape)
print(f"Time (ONNX): {end - start:.4f} s")
```

---
```python
import time
import torch
import onnx
import onnxruntime as ort
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer

# ======================
# 1. Chu·∫©n b·ªã model v√† tokenizer
# ======================
model_name = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
model_torch = SentenceTransformer(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

sentences = [
    "Xin ch√†o, t√¥i ƒëang th·ª≠ benchmark.",
    "PyTorch v√† ONNX kh√°c nhau th·∫ø n√†o?",
    "Ch√∫ng ta s·∫Ω ƒëo t·ªëc ƒë·ªô inference."
]

# ======================
# 2. PyTorch Inference
# ======================
print("üîπ PyTorch Inference")
start = time.time()
embeddings_torch = model_torch.encode(sentences)
end = time.time()
print("Embeddings shape (PyTorch):", embeddings_torch.shape)
print("Time (PyTorch): %.4f s" % (end - start))

# ======================
# 3. Export sang ONNX
# ======================
onnx_path = "model.onnx"

try:
    onnx_model = onnx.load(onnx_path)
    print(f"‚ö° D√πng l·∫°i ONNX model ƒë√£ c√≥: {onnx_path}")
except:
    print("‚ö° Exporting model sang ONNX...")
    # L·∫•y backbone t·ª´ SentenceTransformer v√† ƒë∆∞a v·ªÅ CPU
    model_onnx = model_torch._first_module().auto_model.cpu()

    # Dummy input c≈©ng ph·∫£i ·ªü CPU
    dummy_input = tokenizer("C√¢u th·ª≠ nghi·ªám", return_tensors="pt")
    dummy_input = {
        "input_ids": dummy_input["input_ids"].cpu(),
        "attention_mask": dummy_input["attention_mask"].cpu()
    }

    # Export
    torch.onnx.export(
        model_onnx,
        (dummy_input["input_ids"], dummy_input["attention_mask"]),
        onnx_path,
        input_names=["input_ids", "attention_mask"],
        output_names=["last_hidden_state"],
        dynamic_axes={
            "input_ids": {0: "batch", 1: "sequence"},
            "attention_mask": {0: "batch", 1: "sequence"},
            "last_hidden_state": {0: "batch", 1: "sequence"}
        },
        opset_version=14
    )
    print("‚úÖ Xu·∫•t ONNX th√†nh c√¥ng:", onnx_path)

# ======================
# 4. ONNX Inference
# ======================
print("\nüîπ ONNX Inference")
ort_session = ort.InferenceSession(onnx_path, providers=["CPUExecutionProvider"])

# Tokenize cho ONNX (numpy tensors)
tokens = tokenizer(sentences, padding=True, truncation=True, return_tensors="np")

# ‚ö° Fix: √©p ki·ªÉu sang int64
inputs_onnx = {
    "input_ids": tokens["input_ids"].astype(np.int64),
    "attention_mask": tokens["attention_mask"].astype(np.int64)
}

start = time.time()
outputs = ort_session.run(["last_hidden_state"], inputs_onnx)
end = time.time()

embeddings_onnx = outputs[0][:, 0, :]  # l·∫•y vector [CLS]
print("Embeddings shape (ONNX):", embeddings_onnx.shape)
print("Time (ONNX): %.4f s" % (end - start))




```
---

## 4Ô∏è‚É£ C√°ch ch·∫°y

Trong th∆∞ m·ª•c ch·ª©a script:

```bash
python PyTorchvsONNXRuntime.py
```

---

## 5Ô∏è‚É£ K·∫øt qu·∫£ m·∫´u (th·ª±c t·∫ø Quadro M5000, CPU test)

```
üîπ PyTorch Inference
Embeddings shape (PyTorch): (3, 384)
Time (PyTorch): 1.5623 s

‚ö° D√πng l·∫°i ONNX model ƒë√£ c√≥: model.onnx

üîπ ONNX Inference
Embeddings shape (ONNX): (3, 384)
Time (ONNX): 0.0320 s
```

üëâ **ONNX nhanh h∆°n \~48 l·∫ßn** üöÄ

---

## 6Ô∏è‚É£ Nh·∫≠n x√©t

* **PyTorch**:

  * Linh ho·∫°t cho hu·∫•n luy·ªán & nghi√™n c·ª©u.
  * T√≠ch h·ª£p autograd, debug d·ªÖ.
  * Nh∆∞ng inference ch·∫≠m do overhead.

* **ONNX Runtime**:

  * Chuy√™n d·ª•ng cho inference.
  * T·ªëi ∆∞u kernel cho CPU/GPU/ARM.
  * Nh·∫π, nhanh, d·ªÖ deploy (C++, Java, .NET, Python).
  * H·ªó tr·ª£ quantization (FP16, INT8) ‚Üí t·ªëi ∆∞u th√™m n·ªØa.

---

## 7Ô∏è‚É£ M·ªü r·ªông

Anh c√≥ th·ªÉ th·ª≠:

1. **Benchmark nhi·ªÅu v√≤ng l·∫∑p** (v√≠ d·ª• ch·∫°y 100 l·∫ßn, l·∫•y trung b√¨nh).
2. **Batch l·ªõn** (100 ‚Üí 1000 c√¢u) ƒë·ªÉ xem scaling.
3. **ONNX quantization**:

```bash
pip install onnxruntime-tools
```

```python
from onnxruntime.quantization import quantize_dynamic, QuantType
quantize_dynamic("model.onnx", "model_quant.onnx", weight_type=QuantType.QInt8)
```

‚Üí gi·∫£m dung l∆∞·ª£ng, tƒÉng t·ªëc th√™m.

---

üìå Nh∆∞ v·∫≠y t√†i li·ªáu n√†y ƒë√£ g·ªìm: c√†i ƒë·∫∑t, script ƒë·∫ßy ƒë·ªß, c√°ch ch·∫°y, k·∫øt qu·∫£, ph√¢n t√≠ch, m·ªü r·ªông.

---

Anh c√≥ mu·ªën em vi·∫øt th√™m ph·∫ßn **benchmark nhi·ªÅu v√≤ng (v√≠ d·ª• 100 l·∫ßn ch·∫°y, ƒëo trung b√¨nh v√† ƒë·ªô l·ªách chu·∫©n)** ƒë·ªÉ ƒë∆∞a v√†o b√°o c√°o so s√°nh cho ƒë·∫πp h∆°n kh√¥ng?
