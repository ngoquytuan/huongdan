RÃµ rá»“i ğŸ‘ Em sáº½ soáº¡n cho anh má»™t **tÃ i liá»‡u Ä‘áº§y Ä‘á»§** (kiá»ƒu hÆ°á»›ng dáº«n ká»¹ thuáº­t) Ä‘á»ƒ anh hoáº·c Ä‘á»“ng nghiá»‡p cÃ³ thá»ƒ cháº¡y láº¡i toÃ n bá»™ quÃ¡ trÃ¬nh **so sÃ¡nh PyTorch vs ONNX Runtime**.

---

# ğŸ“˜ HÆ°á»›ng dáº«n: So sÃ¡nh tá»‘c Ä‘á»™ Inference giá»¯a PyTorch vÃ  ONNX Runtime

## 1ï¸âƒ£ Giá»›i thiá»‡u

Trong cÃ¡c há»‡ thá»‘ng AI/ML thá»±c táº¿, PyTorch thÆ°á»ng Ä‘Æ°á»£c dÃ¹ng trong **quÃ¡ trÃ¬nh huáº¥n luyá»‡n** vÃ  tinh chá»‰nh mÃ´ hÃ¬nh. Tuy nhiÃªn khi triá»ƒn khai (deployment), ta cáº§n tá»‘c Ä‘á»™ inference cao vÃ  gá»n nháº¹ â†’ ONNX Runtime lÃ  lá»±a chá»n tá»‘i Æ°u.

TÃ i liá»‡u nÃ y hÆ°á»›ng dáº«n:

* CÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t.
* Viáº¿t script benchmark má»™t mÃ´ hÃ¬nh sentence embedding (`paraphrase-multilingual-MiniLM-L12-v2`).
* So sÃ¡nh tá»‘c Ä‘á»™ inference giá»¯a PyTorch vÃ  ONNX Runtime.

---

## 2ï¸âƒ£ CÃ i Ä‘áº·t mÃ´i trÆ°á»ng

YÃªu cáº§u: Python 3.9+ (nÃªn dÃ¹ng Python 3.10).

```bash
# Táº¡o virtualenv (khuyáº¿n nghá»‹)
python -m venv venvONNX
.\venvONNX\Scripts\activate   # Windows
source venvONNX/bin/activate # Linux/Mac

# CÃ i Ä‘áº·t thÆ° viá»‡n cáº§n thiáº¿t
pip install torch==2.2.2
pip install sentence-transformers
pip install transformers
pip install onnx onnxruntime
```

Kiá»ƒm tra láº¡i:

```bash
python -c "import torch, onnxruntime; print(torch.__version__, onnxruntime.__version__)"
```

---

## 3ï¸âƒ£ Script so sÃ¡nh (PyTorchvsONNXRuntime.py)

LÆ°u file dÆ°á»›i tÃªn `PyTorchvsONNXRuntime.py`:

```python
import time
import torch
import onnx
import onnxruntime as ort
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer

# ----------- Config ------------
model_name = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
onnx_path = "model.onnx"

sentences = [
    "Xin chÃ o, tÃ´i lÃ  AI.",
    "HÃ´m nay trá»i Ä‘áº¹p quÃ¡.",
    "TÃ´i thÃ­ch há»c mÃ¡y."
]

# ----------- PyTorch Inference ------------
model_torch = SentenceTransformer(model_name)
start = time.time()
embeddings_torch = model_torch.encode(sentences, convert_to_numpy=True)
end = time.time()

print("ğŸ”¹ PyTorch Inference")
print("Embeddings shape (PyTorch):", embeddings_torch.shape)
print(f"Time (PyTorch): {end - start:.4f} s")

# ----------- Export ONNX náº¿u chÆ°a cÃ³ ------------
tokenizer = AutoTokenizer.from_pretrained(model_name)
try:
    onnx_model = onnx.load(onnx_path)
    print("âš¡ DÃ¹ng láº¡i ONNX model Ä‘Ã£ cÃ³:", onnx_path)
except:
    print("âš¡ Exporting model sang ONNX...")
    model_onnx = model_torch._first_module().auto_model.cpu()
    dummy_input = tokenizer("CÃ¢u thá»­ nghiá»‡m", return_tensors="pt")
    dummy_input = {
        "input_ids": dummy_input["input_ids"].cpu(),
        "attention_mask": dummy_input["attention_mask"].cpu()
    }
    torch.onnx.export(
        model_onnx,
        (dummy_input["input_ids"], dummy_input["attention_mask"]),
        onnx_path,
        input_names=["input_ids", "attention_mask"],
        output_names=["last_hidden_state"],
        dynamic_axes={
            "input_ids": {0: "batch", 1: "sequence"},
            "attention_mask": {0: "batch", 1: "sequence"},
            "last_hidden_state": {0: "batch", 1: "sequence"}
        },
        opset_version=14
    )
    print("âœ… Xuáº¥t ONNX thÃ nh cÃ´ng:", onnx_path)

# ----------- ONNX Inference ------------
ort_session = ort.InferenceSession(onnx_path, providers=["CPUExecutionProvider"])

tokens = tokenizer(sentences, padding=True, truncation=True, return_tensors="np")
start = time.time()
onnx_outputs = ort_session.run(
    ["last_hidden_state"],
    {"input_ids": tokens["input_ids"], "attention_mask": tokens["attention_mask"]}
)
end = time.time()

embeddings_onnx = onnx_outputs[0].mean(axis=1)  # pooling
print("\nğŸ”¹ ONNX Inference")
print("Embeddings shape (ONNX):", embeddings_onnx.shape)
print(f"Time (ONNX): {end - start:.4f} s")
```

---

## 4ï¸âƒ£ CÃ¡ch cháº¡y

Trong thÆ° má»¥c chá»©a script:

```bash
python PyTorchvsONNXRuntime.py
```

---

## 5ï¸âƒ£ Káº¿t quáº£ máº«u (thá»±c táº¿ Quadro M5000, CPU test)

```
ğŸ”¹ PyTorch Inference
Embeddings shape (PyTorch): (3, 384)
Time (PyTorch): 1.5623 s

âš¡ DÃ¹ng láº¡i ONNX model Ä‘Ã£ cÃ³: model.onnx

ğŸ”¹ ONNX Inference
Embeddings shape (ONNX): (3, 384)
Time (ONNX): 0.0320 s
```

ğŸ‘‰ **ONNX nhanh hÆ¡n \~48 láº§n** ğŸš€

---

## 6ï¸âƒ£ Nháº­n xÃ©t

* **PyTorch**:

  * Linh hoáº¡t cho huáº¥n luyá»‡n & nghiÃªn cá»©u.
  * TÃ­ch há»£p autograd, debug dá»….
  * NhÆ°ng inference cháº­m do overhead.

* **ONNX Runtime**:

  * ChuyÃªn dá»¥ng cho inference.
  * Tá»‘i Æ°u kernel cho CPU/GPU/ARM.
  * Nháº¹, nhanh, dá»… deploy (C++, Java, .NET, Python).
  * Há»— trá»£ quantization (FP16, INT8) â†’ tá»‘i Æ°u thÃªm ná»¯a.

---

## 7ï¸âƒ£ Má»Ÿ rá»™ng

Anh cÃ³ thá»ƒ thá»­:

1. **Benchmark nhiá»u vÃ²ng láº·p** (vÃ­ dá»¥ cháº¡y 100 láº§n, láº¥y trung bÃ¬nh).
2. **Batch lá»›n** (100 â†’ 1000 cÃ¢u) Ä‘á»ƒ xem scaling.
3. **ONNX quantization**:

```bash
pip install onnxruntime-tools
```

```python
from onnxruntime.quantization import quantize_dynamic, QuantType
quantize_dynamic("model.onnx", "model_quant.onnx", weight_type=QuantType.QInt8)
```

â†’ giáº£m dung lÆ°á»£ng, tÄƒng tá»‘c thÃªm.

---

ğŸ“Œ NhÆ° váº­y tÃ i liá»‡u nÃ y Ä‘Ã£ gá»“m: cÃ i Ä‘áº·t, script Ä‘áº§y Ä‘á»§, cÃ¡ch cháº¡y, káº¿t quáº£, phÃ¢n tÃ­ch, má»Ÿ rá»™ng.

---

Anh cÃ³ muá»‘n em viáº¿t thÃªm pháº§n **benchmark nhiá»u vÃ²ng (vÃ­ dá»¥ 100 láº§n cháº¡y, Ä‘o trung bÃ¬nh vÃ  Ä‘á»™ lá»‡ch chuáº©n)** Ä‘á»ƒ Ä‘Æ°a vÃ o bÃ¡o cÃ¡o so sÃ¡nh cho Ä‘áº¹p hÆ¡n khÃ´ng?
